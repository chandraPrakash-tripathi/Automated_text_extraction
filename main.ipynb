{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f4b46aa91fc59c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1.DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T14:28:43.877202900Z",
     "start_time": "2024-06-18T14:28:43.793554700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1998c3b9ce6b6d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2.Function to extract-> article title and text from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4aacbfb20773bc5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T14:29:09.371532700Z",
     "start_time": "2024-06-18T14:28:43.875153800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article blackassign0001 extracted and saved successfully.\n",
      "Article blackassign0002 extracted and saved successfully.\n",
      "Article blackassign0003 extracted and saved successfully.\n",
      "Article blackassign0004 extracted and saved successfully.\n",
      "Article blackassign0005 extracted and saved successfully.\n",
      "Article blackassign0006 extracted and saved successfully.\n",
      "Article blackassign0007 extracted and saved successfully.\n",
      "Article blackassign0008 extracted and saved successfully.\n",
      "Article blackassign0009 extracted and saved successfully.\n",
      "Article blackassign0010 extracted and saved successfully.\n",
      "Article blackassign0011 extracted and saved successfully.\n",
      "Article blackassign0012 extracted and saved successfully.\n",
      "Article blackassign0013 extracted and saved successfully.\n",
      "Failed to extract article blackassign0014.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 55\u001B[0m\n\u001B[0;32m     52\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to extract article \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 55\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[5], line 45\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     42\u001B[0m url \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mURL\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# Extract article title and text\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m ar_title, ar_text \u001B[38;5;241m=\u001B[39m \u001B[43mextract_article\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# Check if extraction was successful\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ar_title \u001B[38;5;129;01mand\u001B[39;00m ar_text:\n",
      "Cell \u001B[1;32mIn[5], line 5\u001B[0m, in \u001B[0;36mextract_article\u001B[1;34m(url)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_article\u001B[39m(url):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 5\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m         soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(response\u001B[38;5;241m.\u001B[39mtext, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      8\u001B[0m         \u001B[38;5;66;03m# removing unwanted elements\u001B[39;00m\n",
      "File \u001B[1;32mE:\\1.Workspace\\3.PROJECTS\\Data_Extraction_&_NLP\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001B[0m, in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\1.Workspace\\3.PROJECTS\\Data_Extraction_&_NLP\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\1.Workspace\\3.PROJECTS\\Data_Extraction_&_NLP\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[0;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[0;32m    587\u001B[0m }\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32mE:\\1.Workspace\\3.PROJECTS\\Data_Extraction_&_NLP\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[0;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[1;32mE:\\1.Workspace\\3.PROJECTS\\Data_Extraction_&_NLP\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[1;32mE:\\1.Workspace\\3.PROJECTS\\Data_Extraction_&_NLP\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[1;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[0;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mE:\\1.Workspace\\3.PROJECTS\\Data_Extraction_&_NLP\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    534\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 536\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[1;32mE:\\1.Workspace\\3.PROJECTS\\Data_Extraction_&_NLP\\.venv\\Lib\\site-packages\\urllib3\\connection.py:464\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n\u001B[0;32m    463\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[1;32m--> 464\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    467\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\http\\client.py:1390\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1388\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1389\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1390\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1391\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[0;32m   1392\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\http\\client.py:325\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 325\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\http\\client.py:286\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 286\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mreadline(_MAXLINE \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[0;32m    288\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\ssl.py:1314\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1310\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1311\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1312\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1313\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1315\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\ssl.py:1166\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1164\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1166\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1167\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1168\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to extract title and text from a URLs \n",
    "\n",
    "def extract_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # removing unwanted elements\n",
    "        for element in soup([\"header\", \"footer\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Extract article title and text\n",
    "        ar_title = soup.find('title').text.strip()\n",
    "        ar_text = \"\"\n",
    "        \n",
    "        # Extract text from <div > tag\n",
    "        ar_div = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "        if ar_div:\n",
    "            ar_text = ar_div.get_text()\n",
    "        return ar_title, ar_text\n",
    "    \n",
    "    except Exception:\n",
    "        print(f\"Error while extracting article from {url}: {Exception}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to save the title and text to a text file\n",
    "    \n",
    "def save_article_to_file(url_id, ar_title, ar_text):\n",
    "    if not os.path.exists(\"articles\"):\n",
    "        os.mkdir(\"articles\")\n",
    "    \n",
    "    with open(f\"articles/{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"Title: {ar_title}\\n\\n\")\n",
    "        file.write(ar_text)\n",
    "\n",
    "def main():\n",
    "    input_file = \"Input.xlsx\"\n",
    "    df = pd.read_excel(input_file)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        url_id = row[\"URL_ID\"]\n",
    "        url = row[\"URL\"]\n",
    "        \n",
    "        # Extract article title and text\n",
    "        ar_title, ar_text = extract_article(url)\n",
    "        \n",
    "        # Check if extraction was successful\n",
    "        if ar_title and ar_text:\n",
    "            save_article_to_file(url_id, ar_title, ar_text)\n",
    "            print(f\"Article {url_id} extracted and saved successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to extract article {url_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Load nltk resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-18T14:29:09.373526400Z"
    }
   },
   "id": "e547b5a2ab5b9020",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.Extracting Dervied Variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f345696cd6da614d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#function to load positive and negative dictionaries\n",
    "def load_both_dictionaries(positive_file, negative_file):\n",
    "    with open(positive_file, \"r\") as file:\n",
    "        positive_words = set(file.read().splitlines())\n",
    "    with open(negative_file, \"r\") as file:\n",
    "        negative_words = set(file.read().splitlines())\n",
    "        \n",
    "    return positive_words, negative_words\n",
    "\n",
    "#func to calculate the metrics of sentimental analysis\n",
    "def sentiment_scores(text,positive_words, negative_words):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    token = word_tokenize(text)\n",
    "    \n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    \n",
    "    for word in token :\n",
    "        #removing punctuation \n",
    "        word = word.lower()\n",
    "        if word.isalpha():\n",
    "            #checking\n",
    "            if word in positive_words:\n",
    "                positive_score+=1\n",
    "            if word in negative_words:\n",
    "                negative_score+=1\n",
    "                \n",
    "    #calculating metrics\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(token) + 0.000001)\n",
    "    \n",
    "    return positive_score,negative_score,polarity_score,subjectivity_score\n",
    "\n",
    "def main():\n",
    "    input_file = \"Output Data Structure.xlsx\"\n",
    "    positive_file = \"positive-words.txt\"\n",
    "    negative_file = \"negative-words.txt\"\n",
    "    article_directory = \"articles\"\n",
    "    \n",
    "    positive_words, negative_words = load_both_dictionaries(positive_file, negative_file)\n",
    "    \n",
    "     #Read output data structure Excel file\n",
    "    output = pd.read_excel(input_file)\n",
    "    \n",
    "    result = []\n",
    "    for index,row in output.iterrows():\n",
    "        url_id = row[\"URL_ID\"]\n",
    "        url = row[\"URL\"]\n",
    "        article_file = os.path.join(article_directory, f\"{url_id}.txt\")\n",
    "        \n",
    "        if os.path.exists(article_file):\n",
    "            #Read article text from file\n",
    "            with open(article_file, 'r', encoding='utf-8') as article:\n",
    "                article_text = article.read()\n",
    "            \n",
    "            # Perform sentiment analysis\n",
    "            positive_score, negative_score, polarity_score, subjectivity_score = sentiment_scores(article_text, positive_words, negative_words)\n",
    "            \n",
    "            result.append({\n",
    "                \"URL_ID\": url_id,\n",
    "                \"URL\": url,\n",
    "                \"Positive_Score\": positive_score,\n",
    "                \"Negative_Score\": negative_score,\n",
    "                \"Polarity_Score\": polarity_score,\n",
    "                \"Subjectivity_Score\": subjectivity_score\n",
    "            })\n",
    "            \n",
    "    \n",
    "     #Creating a DataFrame from result\n",
    "    results_df = pd.DataFrame(result)\n",
    "    \n",
    "    #Save results to Excel\n",
    "    results_df.to_excel(\"sentiment_analysis.xlsx\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T14:29:09.379510700Z",
     "start_time": "2024-06-18T14:29:09.375522800Z"
    }
   },
   "id": "e21274894bd5c551",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sentiment_analysis = pd.read_excel(\"sentiment_analysis.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-18T14:29:09.378513Z"
    }
   },
   "id": "6a5f4811a55f183",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sentiment_analysis"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-18T14:29:09.380508Z"
    }
   },
   "id": "bffbd93ba61bd5a0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T14:29:09.390929Z",
     "start_time": "2024-06-18T14:29:09.383614700Z"
    }
   },
   "id": "bc3a29b01dc13d7b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.Calculating metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b987193df59ade"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to calculate average sentence length\n",
    "def cal_avg_sentence_length(sentences):\n",
    "    total_words = sum(len(word_tokenize(sentence)) for sentence in sentences)\n",
    "    total_sentences = len(sentences)\n",
    "    return total_words / total_sentences\n",
    "\n",
    "# Function to calculate percentage of complex words\n",
    "def cal_percentage_complex_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    complex_words = [word for word in words if len(word) > 2]\n",
    "    return len(complex_words) / len(words)\n",
    "\n",
    "# Function to calculate fog index\n",
    "def cal_fog_index(avg_sentence_length, percentage_complex_words):\n",
    "    return 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "# Function to calculate average number of words per sentence\n",
    "def cal_avg_words_per_sentence(words, sentences):\n",
    "    return len(words) / len(sentences)\n",
    "\n",
    "# Function to calculate complex word count\n",
    "def cal_complex_word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    complex_words = [word for word in words if len(word) > 2]\n",
    "    return len(complex_words)\n",
    "\n",
    "# Function to calculate word count\n",
    "def cal_word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    cleaned_words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "    print(cleaned_words)\n",
    "    return len(cleaned_words)\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    vowels = \"aeiouAEIOU\"\n",
    "    count = 0\n",
    "    if word[-1] in ['e', 'E'] and word[-2:] != 'le' and word[-2:] != 'LE':\n",
    "        word = word[:-1]\n",
    "    for index, letter in enumerate(word):\n",
    "        if index == 0 and letter in vowels:\n",
    "            count += 1\n",
    "        elif letter in vowels and word[index-1] not in vowels:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Function to calculate syllable count per word\n",
    "def cal_syllable_count_per_word(text):\n",
    "    words = word_tokenize(text)\n",
    "    syllable_count = sum(count_syllables(word) for word in words)\n",
    "    return syllable_count / len(words)\n",
    "\n",
    "# Function to calculate personal pronoun count\n",
    "def cal_personal_pronouns(text):\n",
    "    pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "    pattern = r'\\b(?:' + '|'.join(pronouns) + r')\\b'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches)\n",
    "\n",
    "# Function to calculate average word length\n",
    "def cal_avg_word_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    return total_characters / len(words)\n",
    "\n",
    "def main():\n",
    "    output_data_file = \"Output Data Structure.xlsx\"\n",
    "    articles_directory = \"articles\"\n",
    "\n",
    "    # Read output data structure Excel file\n",
    "    output = pd.read_excel(output_data_file)\n",
    "\n",
    "    results_ = []\n",
    "    for index, row in output.iterrows():\n",
    "        url_id = row[\"URL_ID\"]\n",
    "        article_file = os.path.join(articles_directory, f\"{url_id}.txt\")\n",
    "\n",
    "        if os.path.exists(article_file):\n",
    "            # Read article text from file\n",
    "            with open(article_file, 'r', encoding='utf-8') as article:\n",
    "                article_text = article.read()\n",
    "\n",
    "            # Tokenize sentences for text analysis\n",
    "            sentences = sent_tokenize(article_text)\n",
    "            words = word_tokenize(article_text)\n",
    "\n",
    "            # Calculate text analysis metrics\n",
    "            avg_sentence_length = cal_avg_sentence_length(sentences)\n",
    "            percentage_complex_words = cal_percentage_complex_words(article_text)\n",
    "            fog_index = cal_fog_index(avg_sentence_length, percentage_complex_words)\n",
    "            avg_words_per_sentence = cal_avg_words_per_sentence(words, sentences)\n",
    "            complex_word_count = cal_complex_word_count(article_text)\n",
    "            word_count = cal_word_count(article_text)\n",
    "            syllable_count_per_word = cal_syllable_count_per_word(article_text)\n",
    "            personal_pronoun_count = cal_personal_pronouns(article_text)\n",
    "            avg_word_length = cal_avg_word_length(article_text)\n",
    "\n",
    "            results_.append({\n",
    "                \"URL_ID\": url_id,\n",
    "                \"Avg_Sentence_Length\": avg_sentence_length,\n",
    "                \"Percentage_Complex_Words\": percentage_complex_words,\n",
    "                \"Fog_Index\": fog_index,\n",
    "                \"Avg_Words_Per_Sentence\": avg_words_per_sentence,\n",
    "                \"Complex_Word_Count\": complex_word_count,\n",
    "                \"Word_Count\": word_count,\n",
    "                \"Syllable_Count_Per_Word\": syllable_count_per_word,\n",
    "                \"Personal_Pronoun_Count\": personal_pronoun_count,\n",
    "                \"Avg_Word_Length\": avg_word_length\n",
    "            })\n",
    "\n",
    "    # Create DataFrame from results\n",
    "    result_df2 = pd.DataFrame(results_)\n",
    "\n",
    "    # Save results to Excel\n",
    "    result_df2.to_excel(\"Analysis_results.xlsx\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-18T14:29:09.386609400Z"
    }
   },
   "id": "cf4cb6b771ab3c57",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_analysis = pd.read_excel(\"Analysis_results.xlsx\")\n",
    "text_analysis"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-18T14:29:09.389770200Z"
    }
   },
   "id": "a3fc7d356f9bd932",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#merging both sentiment_analysis df and text_analysis df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b02fe49e705d1790"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_df = pd.merge(sentiment_analysis, text_analysis, on='URL_ID')\n",
    "#display dataframe\n",
    "merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T14:29:09.402619500Z",
     "start_time": "2024-06-18T14:29:09.392237Z"
    }
   },
   "id": "fa8c31f0f146b06d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#OUTPUT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6c9294356318603"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_df.to_excel(\"OutputDataStructure.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-18T14:29:09.394552400Z"
    }
   },
   "id": "a32c2a74032db40e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-18T14:29:09.395550700Z"
    }
   },
   "id": "a487031cdc8063e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
